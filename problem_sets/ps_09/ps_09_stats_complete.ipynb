{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 9: Statistics, feature selection, and feature importance (solution)\n",
    "\n",
    "## Summary\n",
    "\n",
    "Examine the differences between British and American fiction in the class-curated literary corpus. Apply statistical measures and calculate feature importance in a simple classifier.\n",
    "\n",
    "## Details\n",
    "\n",
    "You will work with a corpus of 131 volumes of fiction by British and American authors. These volumes are taken from the class corpus, so you'll need to download a [copy of the texts](https://drive.google.com/drive/folders/1lbeZiBAVCzjCWojCK8mfmELa-Q8FMNUm?usp=sharing) and save them somewhere on your machine.\n",
    "\n",
    "You have three tasks for this problem set, all of which depend on comparing British-authored to American-authored books:\n",
    "\n",
    "1. Calculate the mean frequency per 100,000 words, as well as the upper and lower bounds of a 95% confidence interval, for the terms `['color', 'honor', 'center', 'fish', 'person']`\n",
    "    1. Perform this calculation analyticaly, that is, using the observed sample means and standard deviations. I suggest using the `tconfint_mean()` method from the `DescrStatsW()` function provided by the `statsmodels` library. See lecture notes for an example of working code.\n",
    "    1. Calculate the same quantities via bootstrap, using 1,000 or more iterations.\n",
    "    1. In both cases, print your results in a tabular format, like so:\n",
    "\n",
    "```\n",
    "Confidence intervals for: gb\n",
    "     term\t    low\t    mean\t    high\n",
    "   color\t  x.xxxx\t  x.xxxx\t  x.xxxx\n",
    "   [and so on ...]\n",
    "```\n",
    "\n",
    "2. Perform a *t*-test to compare the mean frequency of each of these terms between British and American texts. Report the test statistic and *p*-value for each comparison. Note which means are significantly different at the *p*<0.05 level.\n",
    "3. Perform a logistic regression classification of each volume as British or American. \n",
    "    1. Your final features should be the 25 most informative (as measured by the mutual information criterion) token unigrams.\n",
    "    1. Report your 10-fold cross-validated F1 score before and after restricting your input features to the 25 most-informative token types.\n",
    "    1. Calculate the *importance* of the 25 top features for classification as measured by permutation importance.\n",
    "    \n",
    "* See code stubs below for step-by-step guidance. \n",
    "* Consult, too, the lecture notes on explainability and on statistics.\n",
    "* You'll likely also need to consult the scikit-learn documentation along the way.\n",
    "\n",
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metadata_file = 'amer_brit.csv'\n",
    "corpus_dir = os.path.join('..', '..', 'data', 'classcorpus')\n",
    "terms = ['color', 'honor', 'center', 'fish', 'person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata (5 points)\n",
    "\n",
    "Read the cleaned, minimal corpus metadata from disk (note the variable `metadata_file` defined in the previous cell). I'd suggest using Pandas, but you're welcome to use whatever method you prefer.\n",
    "\n",
    "Note that the format of the metadata file is:\n",
    "\n",
    "```\n",
    "filename,country,wordcount\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>country</th>\n",
       "      <th>wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little_Women_Alcott.txt</td>\n",
       "      <td>us</td>\n",
       "      <td>185902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename country      wc\n",
       "0  Little_Women_Alcott.txt      us  185902"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the corpus metadata\n",
    "corpus = pd.read_csv(metadata_file)\n",
    "\n",
    "# Print the metadata for one volume\n",
    "corpus.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there's a duplicate volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>country</th>\n",
       "      <th>wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Plague_Ship.txt</td>\n",
       "      <td>us</td>\n",
       "      <td>60020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>plague_ship.txt</td>\n",
       "      <td>us</td>\n",
       "      <td>60020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename country     wc\n",
       "87  Plague_Ship.txt      us  60020\n",
       "88  plague_ship.txt      us  60020"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.loc[corpus.filename.str.lower()=='plague_ship.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered this after the problem set went live, so we'll leave it alone. But in the future, we'd want to remove one of the copies. These kinds of problems are typical of real-world corpora. To remove the duplicate, we could either exclude its index position or (more generically) drop any rows with identical properties *other* than filename:\n",
    "\n",
    "```\n",
    "corpus = corpus.drop_duplicates(subset=corpus.columns[1:])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count words and normalize (5 points)\n",
    "\n",
    "* Count the target words (indicated in the problem statement) in each volume. \n",
    "* Then, normalize the count of each word type per 100,000 words in each volume*  I'd suggest using a `CountVectorizer` object, but again, you may approach this task however you like. \n",
    "* Make sure you lowercase the input tokens.\n",
    "* Use the word counts supplied in the metadata file for length normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and normalize the all terms in each volume as indicated\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "file_list = corpus.filename.apply(lambda x: os.path.join(corpus_dir, x))\n",
    "count_data = count_vec.fit_transform(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An alternative, limiting initial counts to just the target terms\n",
    "count_vec = CountVectorizer(\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    vocabulary=terms # <- note this option\n",
    ")\n",
    "\n",
    "file_list = corpus.filename.apply(lambda x: os.path.join(corpus_dir, x))\n",
    "count_data = count_vec.fit_transform(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>country</th>\n",
       "      <th>wc</th>\n",
       "      <th>color</th>\n",
       "      <th>honor</th>\n",
       "      <th>center</th>\n",
       "      <th>fish</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Little_Women_Alcott.txt</td>\n",
       "      <td>us</td>\n",
       "      <td>185902</td>\n",
       "      <td>11.296274</td>\n",
       "      <td>15.599617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.765425</td>\n",
       "      <td>12.372110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The_Great_God_Pan.txt</td>\n",
       "      <td>gb</td>\n",
       "      <td>21639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.242571</td>\n",
       "      <td>13.863857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The_Lost_Kafoozalum.txt</td>\n",
       "      <td>gb</td>\n",
       "      <td>22371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.470073</td>\n",
       "      <td>26.820437</td>\n",
       "      <td>8.940146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filename country      wc      color      honor    center  \\\n",
       "0  Little_Women_Alcott.txt      us  185902  11.296274  15.599617  0.000000   \n",
       "1    The_Great_God_Pan.txt      gb   21639   0.000000   0.000000  0.000000   \n",
       "2  The_Lost_Kafoozalum.txt      gb   22371   0.000000   0.000000  4.470073   \n",
       "\n",
       "        fish     person  \n",
       "0   3.765425  12.372110  \n",
       "1   9.242571  13.863857  \n",
       "2  26.820437   8.940146  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the normalized term frequencies you just calculated for any three documents\n",
    "for term in terms: # select just the target terms from the full matrix\n",
    "    corpus[term] = count_data[:,count_vec.vocabulary_[term]].T.toarray()[0]\n",
    "    corpus[term] = corpus[term]/corpus['wc']*100000\n",
    "corpus.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate analytic means and 95% confidence intervals (15 points)\n",
    "\n",
    "For each of the five indicated terms, calculate and display the mean and 95% confidence interval within each national group. See instructions above for output format.\n",
    "\n",
    "In this part of the problem, calculate your means and CIs analytically, using the observed statistics of each sample, rather than by bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic confidence intervals\n",
      "\n",
      "Confidence intervals for: gb\n",
      "     term\t     low\t    mean\t    high\n",
      "   color\t  0.0573\t  0.6308\t  1.2042\n",
      "   honor\t -0.2501\t  0.4263\t  1.1027\n",
      "   center\t -0.0075\t  0.5890\t  1.1855\n",
      "   fish \t  1.7177\t  4.2106\t  6.7035\n",
      "   person\t 22.3781\t 27.0922\t 31.8064\n",
      "\n",
      "Confidence intervals for: us\n",
      "     term\t     low\t    mean\t    high\n",
      "   color\t  8.6397\t 12.8857\t 17.1316\n",
      "   honor\t  4.6014\t  6.8694\t  9.1373\n",
      "   center\t  4.0506\t  6.4329\t  8.8153\n",
      "   fish \t  3.7917\t  8.4189\t 13.0460\n",
      "   person\t 16.1344\t 20.5517\t 24.9690\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.stats.api as sms\n",
    "g = corpus.groupby('country')\n",
    "print(\"Analytic confidence intervals\")\n",
    "for c, group in g:\n",
    "    print(\"\\nConfidence intervals for:\", c)\n",
    "    print(\"     term\\t     low\\t    mean\\t    high\")\n",
    "    for term in terms:\n",
    "        mean = group[term].mean()\n",
    "        low, high = sms.DescrStatsW(group[term]).tconfint_mean()\n",
    "        print(f'   {term:<5}\\t{low:8.4f}\\t{mean:8.4f}\\t{high:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate bootstrapped means and 95% confidence intervals (15 points)\n",
    "\n",
    "* Calculate the same quantities as above, but this time by bootrap resampling of your data. \n",
    "* Use a minimum of 1,000 trials for each case. \n",
    "* Format your results as in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap confidence intervals\n",
      "\n",
      "Confidence intervals for: gb\n",
      "     term\t     low\t    mean\t    high\n",
      "   color\t  0.1321\t  0.5987\t  1.2706\n",
      "   honor\t  0.0264\t  0.3999\t  1.1557\n",
      "   center\t  0.1310\t  0.5706\t  1.2830\n",
      "   fish \t  2.1171\t  4.1516\t  7.0727\n",
      "   person\t 22.5262\t 27.1448\t 31.6609\n",
      "\n",
      "Confidence intervals for: us\n",
      "     term\t     low\t    mean\t    high\n",
      "   color\t  8.9852\t 12.7413\t 17.4853\n",
      "   honor\t  4.8130\t  6.8344\t  9.2796\n",
      "   center\t  4.3453\t  6.3913\t  8.9628\n",
      "   fish \t  4.4221\t  8.3615\t 13.5650\n",
      "   person\t 16.5477\t 20.5420\t 25.4445\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap calculations\n",
    "trials = 1000\n",
    "print(\"Bootstrap confidence intervals\")\n",
    "for c, group in g:\n",
    "    print(\"\\nConfidence intervals for:\", c)\n",
    "    print(\"     term\\t     low\\t    mean\\t    high\")\n",
    "    for term in terms:\n",
    "        bootstrapped_means = []\n",
    "        k = group[term].count() # Number of objects per sample\n",
    "        for i in range(trials):\n",
    "            sample = group[term].sample(n=k, replace=True)\n",
    "            bootstrapped_means.append(np.mean(sample))\n",
    "        result = sorted(bootstrapped_means)\n",
    "        mean = result[int(trials/2)]\n",
    "        low = result[int(trials*0.025)]  # 2.5%\n",
    "        high = result[int(trials*0.975)] # 97.5%\n",
    "        print(f'   {term:<5}\\t{low:8.4f}\\t{mean:8.4f}\\t{high:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *t*-tests (20 points)\n",
    "\n",
    "* Perform a *t*-test comparing the mean frequency of each of the indicated terms in the British and American subsets of the corpus.\n",
    "    * You will perform 5 total tests, comparing, for example, the mean frequency of `color` in British texts to the mean frequency of `color` in American texts. Do not cross-compare words (that is, don't compare the frequency of `color` to that of `honor`, etc.).\n",
    "* Display the test statistic and *p*-value for each comparison. \n",
    "    * Format your output for easy readability (do not just print the raw `ttest_ind` object).\n",
    "* Note which differences are significant at the *p*<0.05 level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term\tstatistic\t  p-value\n",
      "color\t    5.711\t2.662e-07\n",
      "honor\t    5.436\t6.096e-07\n",
      "center\t    4.751\t9.649e-06\n",
      "fish\t    1.599\t    0.113\n",
      "person\t   -2.022\t  0.04523\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "print(\"term\\tstatistic\\t  p-value\")\n",
    "for term in terms:\n",
    "    stat, p = ttest_ind(\n",
    "        corpus.loc[corpus.country=='us'][term], \n",
    "        corpus.loc[corpus.country=='gb'][term],\n",
    "        equal_var=False\n",
    "    )\n",
    "    print(f\"{term}\\t{stat:9.4}\\t{p:9.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Significant:** color, honor, center, person (*marginal*)\n",
    "* **Not sigificant:** fish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection (25 points)\n",
    "\n",
    "* Vectorize the corpus as indicated below (freebie)\n",
    "* Standard-scale the resulting feature matrix\n",
    "* Produce a one-dimensional label vector, y, indicating the national origin of each volume in the corpus\n",
    "    * Use `1` to indicate American, `0` for British\n",
    "* Calculate the 10-fold cross-validated classification accuracy and F1 score using a logistic regression classifier on the full input matrix\n",
    "* From the full matrix, select the 25 most-informative features\n",
    "    * Use sklearn's `SelectKBest` function with the  `mutual_info_classif` scoring function to produce a feature matrix that contains just these 25 most-informative features\n",
    "    * Print a list of the names (token labels; for example, 'color') of these 25 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (131, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize (freebie)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def pre_proc(x):\n",
    "    '''\n",
    "    Takes a unicode string.\n",
    "    Lowercases, strips accents, and removes some escapes.\n",
    "    Returns a standardized version of the string.\n",
    "    '''\n",
    "    import unicodedata\n",
    "    return unicodedata.normalize('NFKD', x.replace(\"_\", \" \").lower().strip())\n",
    "\n",
    "# Set up vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    input='filename',\n",
    "    encoding='utf-8',\n",
    "    preprocessor=pre_proc,\n",
    "    min_df=11, # Note this\n",
    "    max_df=0.8, # This, too\n",
    "    binary=False,\n",
    "    norm='l2',\n",
    "    max_features=5000,\n",
    "    use_idf=True # And this\n",
    ")\n",
    "\n",
    "# Perform vectorization\n",
    "X = vectorizer.fit_transform(file_list) # <-- MODIFY TO USE THE LIST OF FILES ON YOUR MACHINE\n",
    "\n",
    "# Get the dimensions of the doc-term matrix\n",
    "print(\"Matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scaled value: -6.942707647121589e-18\n"
     ]
    }
   ],
   "source": [
    "# Standard-scale your feature matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X.todense())\n",
    "\n",
    "# Print the mean of your scaled features.\n",
    "# Should be very close to zero.\n",
    "print(\"Mean scaled value:\", np.mean(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of US texts: 67\n"
     ]
    }
   ],
   "source": [
    "# Produce a one-dimensional vector of true labels for classification\n",
    "# 1='us', 0='gb'\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "y = LabelBinarizer().fit_transform(corpus.country.to_numpy().reshape(-1, 1)).ravel()\n",
    "\n",
    "# Using your label vector, display the number of US texts in the corpus\n",
    "print(\"Number of US texts:\", np.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate the logistic regression classifier on full input data\n",
    "# Consult PS 6 for leads\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Classifers to test\n",
    "classifiers = {\n",
    "    'Logit':LogisticRegression()\n",
    "}\n",
    "\n",
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X, # feature matrix\n",
    "        y, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy', 'f1'] # scoring methods\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the performance of our classifier\n",
    "# Freebie function to summarize and display classifier scores\n",
    "def compare_scores(scores_dict, color=True):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean)\n",
    "    if color:\n",
    "        df = df.style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logit</th>\n",
       "      <td>0.06704</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.847802</td>\n",
       "      <td>0.843337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fit_time  score_time  test_accuracy   test_f1\n",
       "Logit   0.06704    0.001427       0.847802  0.843337"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare cross-validation scores\n",
    "# Note that colorization of the `time` columns is counterintuitive\n",
    "compare_scores(scores, color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the new feature matrix: (131, 25)\n"
     ]
    }
   ],
   "source": [
    "# Select the 25 most-informative features as specified above \n",
    "#  and produce a new feature matrix containing only those features\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "selector = SelectKBest(mutual_info_classif, k=25)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Print the shape of your new feature matrix\n",
    "print(\"Shape of the new feature matrix:\", X_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afterwards',\n",
       " 'british',\n",
       " 'color',\n",
       " 'colored',\n",
       " 'coloured',\n",
       " 'endeavoured',\n",
       " 'endeavouring',\n",
       " 'events',\n",
       " 'extraordinary',\n",
       " 'favor',\n",
       " 'favour',\n",
       " 'grey',\n",
       " 'honour',\n",
       " 'honourable',\n",
       " 'horizon',\n",
       " 'humour',\n",
       " 'inn',\n",
       " 'london',\n",
       " 'neighbourhood',\n",
       " 'realise',\n",
       " 'remainder',\n",
       " 'sore',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'traveling']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the names of the features retained in the new feature matrix\n",
    "# Store these feature names in a list, then print the list\n",
    "\n",
    "# Hint: use a combination of your original vectorizer's `.get_feature_names()` method \n",
    "#  and the `SelectKBest` object's `.get_support()` method\n",
    "feature_names = [x for i, x in enumerate(vectorizer.get_feature_names()) if selector.get_support()[i]]\n",
    "display(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logit</th>\n",
       "      <td>0.008607</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>0.878473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fit_time  score_time  test_accuracy   test_f1\n",
       "Logit  0.008607    0.001269       0.878571  0.878473"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display the 10-fold cross-validated accuracy and F1 of the\n",
    "#  logistic regression using the new, smaller feature matrix\n",
    "scores = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X_selected, # feature matrix\n",
    "        y, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy', 'f1'] # scoring methods\n",
    "    )\n",
    "compare_scores(scores, color=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the 5 most important features (15 points)\n",
    "\n",
    "* Split the new matrix of most-informative features into train (75%) and test (25%) sets (use sklearn's `train_test_split`)\n",
    "* Train a default logistic regression classifier on the training set\n",
    "    * Print the trained model's score on the test set (use the trained classifier's `.score()` method)\n",
    "* Use sklearn's `permutation_importance` function to calculate the importance of each input feature\n",
    "* Print the feature importances from most to least important using the supplied function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9393939393939394\n"
     ]
    }
   ],
   "source": [
    "# Split the selected feature matrix into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y)\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Test score:\", model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance via permutation\n",
    "from sklearn.inspection import permutation_importance\n",
    "r = permutation_importance(model, X_val, y_val, n_repeats=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_importances(importance_object, feature_names):\n",
    "    '''\n",
    "    Takes a trained permutation_importance object and a list of feature names.\n",
    "    Prints an ordered list of features by descending importance.\n",
    "    '''\n",
    "    for i in importance_object.importances_mean.argsort()[::-1]:\n",
    "\n",
    "        print(f\"{feature_names[i]:<8}\"\n",
    "            f\"\\t{importance_object.importances_mean[i]:.3f}\"\n",
    "            f\" +/- {importance_object.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "british \t0.054 +/- 0.030\n",
      "toward  \t0.050 +/- 0.032\n",
      "afterwards\t0.041 +/- 0.034\n",
      "color   \t0.038 +/- 0.035\n",
      "honour  \t0.016 +/- 0.029\n",
      "endeavoured\t0.015 +/- 0.018\n",
      "inn     \t0.013 +/- 0.018\n",
      "favor   \t0.013 +/- 0.023\n",
      "horizon \t0.011 +/- 0.017\n",
      "remainder\t0.006 +/- 0.013\n",
      "extraordinary\t0.003 +/- 0.008\n",
      "endeavouring\t0.001 +/- 0.005\n",
      "grey    \t0.001 +/- 0.005\n",
      "sore    \t0.000 +/- 0.000\n",
      "events  \t0.000 +/- 0.000\n",
      "realise \t0.000 +/- 0.000\n",
      "neighbourhood\t0.000 +/- 0.000\n",
      "london  \t0.000 +/- 0.000\n",
      "traveling\t0.000 +/- 0.000\n",
      "towards \t-0.001 +/- 0.018\n",
      "favour  \t-0.002 +/- 0.013\n",
      "colored \t-0.002 +/- 0.013\n",
      "coloured\t-0.002 +/- 0.013\n",
      "humour  \t-0.002 +/- 0.008\n",
      "honourable\t-0.017 +/- 0.015\n"
     ]
    }
   ],
   "source": [
    "# Print ranked list of features by permutation importance\n",
    "print_importances(r, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
