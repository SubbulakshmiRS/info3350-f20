{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 December 2020**\n",
    "\n",
    "# Neural networks\n",
    "\n",
    "## Admin\n",
    "\n",
    "* Spring TA applications close **today**\n",
    "    * https://cis-student-hiring.coecis.cornell.edu/\n",
    "    * Applications are brief and the experience is rewarding ... and you get paid\n",
    "* Problem set 11 (the last one!)\n",
    "    * Due Tuesday, 12/8, at 11:59pm\n",
    "* Read Antoniak et al. for Wednesday\n",
    "    * Antoniak et al. use social media data to study birth narratives. Maria will lead a discussion of their research and of the ethical and practical issues involved.\n",
    "        * See also the optional reading, on the ethics of using data from fan communities.\n",
    "    * Fourth and final reading response due by the evening of Tuesday, 12/15\n",
    "    * If responding in the final week, task will be to reflect on the range of methods and problems covered in the Wednesday readings over the course of the semester. This is somewhat longer and more difficult than a standard reading response.\n",
    "* Friday section will be open project consultation and group work.\n",
    "    * Attendance optional\n",
    "    * Contemplating trying gather.town. Details to come via Campuswire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks and deep learning\n",
    "\n",
    "* We've used NLP tools at many points this semester, but this isn't an NLP class\n",
    "* That said, neural methods have transformed many areas of NLP over the last decade\n",
    "    * And deep learning -- a subset of neural methods -- has been very widely applied in machine learning and AI\n",
    "* Our tasks today: define \"neural network,\" relate neural nets to other learning systems, take a look at how a neural network works, and show how to implement a very simple neural classifier in Python\n",
    "\n",
    "### What is a neural network?\n",
    "\n",
    "* A neural network is a computing system comprising artificial neurons\n",
    "* Neurons were originally (1940s) intended to model organic brain behavior\n",
    "    * But now, the name is really just a bit of jargon. No one thinks its important whether or not computational neurons have anything to do with biological neurons.\n",
    "* Individual neurons are mathematical functions that take a vector of input values and produce a single output value.\n",
    "    * We've seen lots of these kinds of functions over the semester, not all of them related to actually existing neural networks\n",
    "    * What matters are the details of the functions and the ways they relate to one another in a network\n",
    "* In a neural network, the neurons are connected to one another in one or more layers, so that the output of one neuron is the input of another (or many others)\n",
    "    \n",
    "### Logistic regression\n",
    "\n",
    "* Logistic regression **is not a neural network** in the modern sense, but it captures much of the spirit of a basic neural network and a lot of the math is related, so let's revisit it\n",
    "* Fit training data to a linear model: $z = W_0 + W_1 x_1 + W_2 x_2 + ...$\n",
    "    * Values of $x$ are observed properties of an object (counts of individual words, say)\n",
    "    * The $W$s are weights. We multiply the weight associated with each word (for example) by the number of times that word occurs in a document.\n",
    "        * These types of element-wise multiplications between two vectors are called **dot products**\n",
    "    * Add up the weight * count products and we produce an output value, $z$\n",
    "    * Note that values of $z$ can range from -infinity to +infinity\n",
    "* Transform the linear value into a score between 0 and 1 using the sigmoid function: $$\\sigma(t) = \\frac{1}{1 + e^{-z}}$$\n",
    "* Sigmoid function looks like this:\n",
    "    \n",
    "<img src=\"./images/sigmoid.png\">\n",
    "\n",
    "* When we train a logistic regression classifier, we're trying to learn the set of weights that produce the most accurate classifications\n",
    "* We learn the weights by:\n",
    "    * Initializing to random values (or equal values, or some arrangement that reflects our best guess about the correct weights)\n",
    "    * Calculating **cross-entropy loss**, that is, how far away are our predicted outputs from the known-true (gold) values.\n",
    "        * Our goal is to minimize this loss function by adjusting the weights in our model\n",
    "        * See Jurafsky and Martin, ch. 5, for the math, but the short version is that we take (roughly) the negative log of the sum of the differences between the predicted labels (as probabilities ranging from 0 to 1) and the true labels (which are either 0 or 1)\n",
    "        * Trivia point: logistic regression is a more advanced version of the **perceptron** (which uses a binary loss function rather than a probabilistic one). The perceptron was invented at Cornell (by Frank Rosenblatt in 1958).\n",
    "    * Adjusting our weights using **gradient descent**\n",
    "        * Again, the math isn't important to us, but ... we find the gradient (slope) of the loss function by partial differentiation with respect to each of the weights. In short, we find how the loss function chages in response to small changes in each weight, then move the weight in the direction that minimizes the loss. Repeat until the loss function stops changing (much) and hope we've found the global minimum (that is, the globally best weights).\n",
    "* If you've been around neural networks and machine learning, these terms will sound familiar: loss function, gradient descent. Now you know what they mean.\n",
    "\n",
    "### From logistic regression to feed-forward networks\n",
    "\n",
    "* The problem with logistic regression (which is a great classifier for many problems!) is that it can only learn linear relationships between inputs and outputs. If our problem is nonlinear, logistic regression might not work well on it.\n",
    "* The simplest way to understand the relationship between logistic regression and a basic neural network is that a neural network is made up of multiple logistic-like functions, each of which can learn a different part of the correct solution (where \"solution\" = function that best fits the training data)\n",
    "* Here's a schematic representation (from Jurafsky and Martin) of a feed-forward network with a single hidden layer (the middle one, with labels $h_i$):\n",
    "\n",
    "<img src=\"./images/neural_network.png\">\n",
    "\n",
    "* There are three layers here: input, hidden, and output.\n",
    "    * The input layer is the data you feed into the system.\n",
    "    * The hidden layer is where the weights are adjusted to maximize classification accuracy. This is what *learns*.\n",
    "    * The output layer translates numerical values calculated in the hidden layer into class probabilities (that is, into specific classification decisions).\n",
    "* The math in this case is the same as in the logistic case, except that:\n",
    "    * We have matrices of weights across the neurons, rather than a single vector of weights for a single neuron\n",
    "    * We have a vector of outputs from the hidden layer, rather a single, scalar output\n",
    "    * Gradient descent is harder, because there are more paths to differentiate\n",
    "        * This is the most consequential difference in practical terms, because it really slows down training\n",
    "        * The standard approach is **backpropagation**. For details, See Jurafsky and Martin, ch. 7. It's like partial differentiation, but performed piece-wise backward through the all the possible paths from outputs to inputs via the hidden layer(s). \n",
    "        \n",
    "### From shallow to deep\n",
    "\n",
    "* Even a neural network with a single hidden layer (of possibly infinite width; that is, made up of arbitrarily many neurons) can be shown to be able to represent a function of arbirary complexity\n",
    "    * Note in passing: this is a remarkable result. It means that neural networks are immensely flexible in the relationships between inputs and outputs that they can model.\n",
    "    * But this fact doesn't imply that it's *easy* to learn a correct or high-performing representation of an arbitrary function in a neural network\n",
    "* In practice, it can be more efficient to build networks that are narrower but *deeper*; that have more layers\n",
    "* Deep learning also largely removes the need for (ceratin kinds of) feature engineering, since the layers learn maximally effective transformations of the data\n",
    "    * But the right kinds of data still need to be present in the first place!\n",
    "    * If you only give your network word counts, it won't magically engineer paratextual features.\n",
    "* You may have heard of **convolutional** neural networks and **recurrent** neural networks. These are networks in which there is not a strict one-to-one connection between all the neurons in each layer.\n",
    "    * Convolutional networks are widely used in image recognition\n",
    "    * Recurrent networks (in which parts of layers are connected both forward and backward) are often used in NLP applications\n",
    "* All of this is **bloody slow** and involves a lot of matrix math. Two main factors have driven the deep learning revolution over the last decade:\n",
    "    * Web-scale data, which provides enough instances to learn fine distinctions in complex decision boundaries\n",
    "        * A method that can model arbitrarily complex functions isn't much good if you don't have enough data to explore the function space\n",
    "    * GPUs (graphics cards), which are essentially super-fast matrix calculators\n",
    "        * These make computing with all that data tractable (more or less)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic neural network classification in `sklearn`\n",
    "\n",
    "The only neural classifier built into `sklearn` is the multi-layer perceptron (which isn't really a perceptron at all, but the name stuck). We'll demonstrate it here; it's easy and works as a drop-in replacement for any other classifier.\n",
    "\n",
    "For more adavnced work with neural networks, you'd want to explore frameworks like [Keras](https://keras.io/) and [PyTorch](https://pytorch.org/), which are more flexible and support computations on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding representation of reviews data\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from   sklearn.linear_model import LogisticRegression\n",
    "from   sklearn.model_selection import cross_val_score\n",
    "from   sklearn.neural_network import MLPClassifier\n",
    "from   sklearn.preprocessing import StandardScaler\n",
    "\n",
    "with open(os.path.join('supplements', 'X_embed.pickle'), 'rb') as f:\n",
    "    X = pickle.load(f) # embedding-based\n",
    "with open(os.path.join('supplements', 'X_tfidf.pickle'), 'rb') as f:\n",
    "    X_tfidf = pickle.load(f) # token-based\n",
    "with open(os.path.join('supplements', 'y.pickle'), 'rb') as f:\n",
    "    y = pickle.load(f) # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18362, 300)\n"
     ]
    }
   ],
   "source": [
    "# scale\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit accuracy: 0.7009037420213571\n",
      "CPU times: user 8.12 s, sys: 144 ms, total: 8.26 s\n",
      "Wall time: 4.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# logit score\n",
    "logit_scores = cross_val_score(LogisticRegression(max_iter=1000), X, y, cv=5)\n",
    "print(\"Logit accuracy:\", np.mean(logit_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy: 0.6618566741819093\n",
      "CPU times: user 57 ms, sys: 112 ms, total: 169 ms\n",
      "Wall time: 50.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MLP score, no optimization\n",
    "mlpc = MLPClassifier()\n",
    "mlp_scores = cross_val_score(mlpc, X, y, cv=5, n_jobs=-1)\n",
    "print(\"MLP accuracy:\", np.mean(mlp_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great! It's slower and performs worse than logistic regression. Recall, for comparison, that our token-based logistic regression score (from the problem set) was around 0.66.\n",
    "\n",
    "Let's try some tuning (tuning neural networks is super important) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 s, sys: 54.7 ms, total: 2.66 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Grid search: wide vs. deep, and compare solvers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(300,), (100,), (10,), (2,), (100,10), (30,10), (10,2)],\n",
    "    'solver':['adam', 'lbfgs'],\n",
    "}\n",
    "clf = GridSearchCV(mlpc, params, n_jobs=-1)\n",
    "\n",
    "with warnings.catch_warnings() as w:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    clf.fit(X[:2000], y[:2000]) # Note subset of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layer_sizes': (10,), 'solver': 'adam'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which parameters are best?\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the cv score of the best classifier?\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy (tuned): 0.7024287223435219\n",
      "CPU times: user 39.5 ms, sys: 50 ms, total: 89.5 ms\n",
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Score after tuning\n",
    "mlp_tuned_scores = cross_val_score(\n",
    "    MLPClassifier(max_iter=500, **clf.best_params_), \n",
    "    X, \n",
    "    y, \n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"MLP accuracy (tuned):\", np.mean(mlp_tuned_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy (using tokens): 0.7222520429655965\n",
      "CPU times: user 2.79 s, sys: 2.37 s, total: 5.17 s\n",
      "Wall time: 3min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.4min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compare the untuned, token-based version\n",
    "mlp_tfidf_scores = cross_val_score(\n",
    "    mlpc, \n",
    "    StandardScaler().fit_transform(X_tfidf.toarray()), \n",
    "    y, \n",
    "    cv=5, \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"MLP accuracy (using tokens):\", np.mean(mlp_tfidf_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
